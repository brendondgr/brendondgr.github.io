<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Llama CPP Wrapper - brendondgr</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.2/font/bootstrap-icons.css">
    <link rel="stylesheet" href="/section/css/main.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #0d1117; color: #c9d1d9; }
        .accent-glow { color: #58a6ff; text-shadow: 0 0 10px rgba(88, 166, 255, 0.5); }
        .panel { background: rgba(22, 27, 34, 0.8); backdrop-filter: blur(8px); border: 1px solid rgba(48, 54, 61, 0.5); border-radius: 12px; }
        .section-title { font-family: 'Fira Code', monospace; }
        .capability-card:hover { border-color: #58a6ff; transform: translateY(-4px); transition: all 0.3s ease; }
        .step-number { font-family: 'Fira Code', monospace; width: 40px; height: 40px; display: flex; align-items: center; justify-content: center; border-radius: 50%; background: #238636; color: white; font-weight: bold; flex-shrink: 0; }
        .hero-gradient { background: linear-gradient(180deg, rgba(13, 17, 23, 0) 0%, rgba(13, 17, 23, 1) 100%); }
        .screenshot-container { border: 1px solid rgba(48, 54, 61, 0.5); border-radius: 8px; overflow: hidden; background: #010409; }
        .screenshot-label { font-family: 'Fira Code', monospace; font-size: 0.8rem; padding: 0.5rem 1rem; background: rgba(48, 54, 61, 0.5); color: #8b949e; }
        .integration-link { transition: all 0.2s ease; border: 1px solid transparent; }
        .integration-link:hover { background: rgba(88, 166, 255, 0.1); border-color: rgba(88, 166, 255, 0.3); transform: scale(1.02); }
    </style>
</head>
<body class="antialiased pt-16">
    <div id="navbar-container"></div>

    <div class="relative w-full h-[60vh] overflow-hidden">
        <img src="../images/locallmm_hero.png" alt="Llama CPP Wrapper Hero" class="w-full h-full object-cover">
        <div class="absolute inset-0 hero-gradient"></div>
        <div class="absolute bottom-0 left-0 right-0 p-8 md:p-16 text-center">
            <h1 class="text-5xl md:text-7xl font-bold text-white mb-4 tracking-tight">Llama CPP Wrapper</h1>
            <p class="text-xl md:text-2xl text-slate-300 max-w-3xl mx-auto font-light">A sophisticated Python orchestration layer for local LLM deployment and management.</p>
        </div>
    </div>

    <div class="container mx-auto p-4 md:p-8 max-w-6xl">
        
        <!-- Overview Section -->
        <section class="mb-20">
            <div class="panel p-8 md:p-12">
                <h2 class="section-title text-2xl font-bold mb-6 text-white"><span class="accent-glow">//</span> Overview</h2>
                <p class="text-lg text-slate-400 leading-relaxed mb-8">
                    The <strong>Llama CPP Wrapper</strong> is designed to simplify the deployment and management of Large Language Models on local hardware. It acts as a robust bridge between high-performance backends like <code class="bg-slate-800 px-2 py-1 rounded">llama.cpp</code> and user-facing interfaces. It supports <strong>any form of GGUF</strong> model, allowing for maximum flexibility in choosing the right model for your specific needs.
                </p>
                <div class="flex flex-wrap gap-4">
                    <a href="https://github.com/brendondgr/LLM-Local" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-6 py-3 bg-[#238636] hover:bg-[#2ea043] text-white rounded-md font-semibold transition-colors">
                        <i class="fab fa-github mr-2"></i> View on GitHub
                    </a>
                    <div class="inline-flex items-center px-4 py-2 border border-slate-700 rounded-md text-slate-400 text-sm">
                        <i class="fas fa-server mr-2"></i> Accessible via Localhost
                    </div>
                </div>
            </div>
        </section>

        <!-- Visual Showcase (Stacked Layout) -->
        <section class="mb-20">
            <h2 class="section-title text-3xl font-bold mb-10 text-white text-center">System Interface</h2>
            <div class="flex flex-col gap-20 max-w-5xl mx-auto">
                <!-- Frontend Screenshot -->
                <div class="space-y-8 flex flex-col items-center text-center">
                    <div class="max-w-3xl">
                        <h3 class="text-2xl font-bold text-white mb-3">Modular Web Interface</h3>
                        <p class="text-slate-400 leading-relaxed">
                            The central frontend is designed for modularity and easy adoption. It can be seamlessly integrated into any project being developed with <strong>Flask</strong> or <strong>Django</strong>, providing a consistent chat and management experience across different web ecosystems.
                        </p>
                    </div>
                    <div class="screenshot-container shadow-2xl w-full">
                        <div class="screenshot-label">frontend.png</div>
                        <img src="../images/LlamaCPPWrapper/frontend.png" alt="Llama CPP Wrapper Frontend" class="w-full h-auto">
                    </div>
                </div>
                <!-- Menu Screenshot -->
                <div class="space-y-8 flex flex-col items-center text-center">
                    <div class="max-w-3xl">
                        <h3 class="text-2xl font-bold text-white mb-3">Advanced Configuration Menu</h3>
                        <p class="text-slate-400 leading-relaxed">
                            Load specific language and voice models directly from the menu. Fine-tune execution with precision controls for <strong>text-streaming</strong>, <strong>context size</strong>, and <strong>max tokens</strong>. These configurations are persistent, ensuring your preferred setup is preserved across sessions.
                        </p>
                    </div>
                    <div class="screenshot-container shadow-2xl max-w-2xl w-full">
                        <div class="screenshot-label">menu.png</div>
                        <img src="../images/LlamaCPPWrapper/menu.png" alt="Llama CPP Wrapper Menu" class="w-full h-auto">
                    </div>
                </div>
            </div>
        </section>

        <!-- Integrations Section -->
        <section class="mb-20">
            <h2 class="section-title text-3xl font-bold mb-10 text-white text-center">Integrations & Ecosystem</h2>
            <div class="panel p-8">
                <p class="text-slate-400 mb-10 text-center italic text-lg">
                    The Llama CPP Wrapper serves as the foundational intelligence layer for various projects across my portfolio.
                </p>
                <div class="space-y-4 max-w-4xl mx-auto">
                    <!-- Petunia -->
                    <a href="petunia.html" class="panel p-6 integration-link flex items-center gap-6 group">
                        <img src="https://placehold.co/100x100/334155/FFFFFF?text=P" class="w-16 h-16 rounded-lg object-cover border border-slate-700" alt="Petunia">
                        <div class="flex-grow">
                            <h3 class="text-xl font-bold text-white group-hover:text-sky-400 transition-colors">Petunia</h3>
                            <p class="text-slate-400 text-sm">A CharacterAI clone utilizing the Llama CPP Wrapper for persistent, high-fidelity personality-driven agents.</p>
                        </div>
                        <i class="bi bi-arrow-right text-slate-600 group-hover:text-sky-400 transition-colors text-2xl"></i>
                    </a>
                    <!-- Fellowship Finder -->
                    <a href="fellowship.html" class="panel p-6 integration-link flex items-center gap-6 group">
                        <img src="../images/fellowshipfinder.png" class="w-16 h-16 rounded-lg object-cover border border-slate-700" alt="Fellowship Finder" onerror="this.src='https://placehold.co/100x100/334155/FFFFFF?text=FF'">
                        <div class="flex-grow">
                            <h3 class="text-xl font-bold text-white group-hover:text-sky-400 transition-colors">Fellowship Finder</h3>
                            <p class="text-slate-400 text-sm">Leverages local inference to refine and categorize hundreds of scraped fellowship opportunities automatically.</p>
                        </div>
                        <i class="bi bi-arrow-right text-slate-600 group-hover:text-sky-400 transition-colors text-2xl"></i>
                    </a>
                    <!-- Magnification Job Finder -->
                    <a href="magnification_job_finder.html" class="panel p-6 integration-link flex items-center gap-6 group">
                        <img src="https://placehold.co/100x100/334155/FFFFFF?text=MJ" class="w-16 h-16 rounded-lg object-cover border border-slate-700" alt="Magnification Job Finder">
                        <div class="flex-grow">
                            <h3 class="text-xl font-bold text-white group-hover:text-sky-400 transition-colors">Magnification Job Finder</h3>
                            <p class="text-slate-400 text-sm">Uses the Llama CPP Wrapper to parse job descriptions and identify niche roles within the magnification and optics industry.</p>
                        </div>
                        <i class="bi bi-arrow-right text-slate-600 group-hover:text-sky-400 transition-colors text-2xl"></i>
                    </a>
                    <!-- Project Peach -->
                    <a href="project_peach.html" class="panel p-6 integration-link flex items-center gap-6 group">
                        <img src="https://placehold.co/100x100/334155/FFFFFF?text=PP" class="w-16 h-16 rounded-lg object-cover border border-slate-700" alt="Project Peach">
                        <div class="flex-grow">
                            <h3 class="text-xl font-bold text-white group-hover:text-sky-400 transition-colors">Project Peach</h3>
                            <p class="text-slate-400 text-sm">Integrates LLM-driven natural language parsing for calendar event creation and personalized scheduling suggestions.</p>
                        </div>
                        <i class="bi bi-arrow-right text-slate-600 group-hover:text-sky-400 transition-colors text-2xl"></i>
                    </a>
                    <!-- Project Mango -->
                    <div class="panel p-6 flex items-center gap-6 border-dashed border-slate-700 bg-slate-800/20">
                        <img src="https://placehold.co/100x100/334155/FFFFFF?text=PM" class="w-16 h-16 rounded-lg object-cover border border-slate-700 opacity-50" alt="Project Mango">
                        <div class="flex-grow">
                            <div class="flex items-center gap-3">
                                <h3 class="text-xl font-bold text-slate-500">Project Mango</h3>
                                <span class="bg-amber-600/20 text-amber-500 border border-amber-600/30 px-2 py-0.5 rounded text-[10px] font-bold uppercase tracking-wider">Coming Soon</span>
                            </div>
                            <p class="text-slate-500 text-sm">Designed as a privacy-first autonomous agent running entirely on the Llama CPP Wrapper orchestration layer.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Core Capabilities Grid -->
        <section class="mb-20">
            <h2 class="section-title text-3xl font-bold mb-10 text-white text-center">Core Capabilities</h2>
            <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-6">
                <!-- Cap 1 -->
                <div class="panel p-6 capability-card">
                    <div class="text-sky-400 text-3xl mb-4"><i class="bi bi-file-earmark-binary"></i></div>
                    <h3 class="text-xl font-bold text-white mb-3">GGUF Universal Support</h3>
                    <p class="text-slate-400 text-sm leading-relaxed">
                        Fully compatible with any GGUF quantized models. Easily switch between sizes and architectures to fit your hardware constraints.
                    </p>
                </div>
                <!-- Cap 2 -->
                <div class="panel p-6 capability-card">
                    <div class="text-sky-400 text-3xl mb-4"><i class="bi bi-cpu"></i></div>
                    <h3 class="text-xl font-bold text-white mb-3">Hardware Abstraction</h3>
                    <p class="text-slate-400 text-sm leading-relaxed">
                        Automatic GPU detection and optimal "Farm" selection. Handles GPU layer offloading and CPU thread allocation dynamically.
                    </p>
                </div>
                <!-- Cap 3 -->
                <div class="panel p-6 capability-card">
                    <div class="text-sky-400 text-3xl mb-4"><i class="bi bi-chat-dots"></i></div>
                    <h3 class="text-xl font-bold text-white mb-3">Session Management</h3>
                    <p class="text-slate-400 text-sm leading-relaxed">
                        Maintains client-side chat history and manages KV Cache Slots for fast multi-turn conversations via pre-computed data reuse.
                    </p>
                </div>
                <!-- Cap 4 -->
                <div class="panel p-6 capability-card">
                    <div class="text-sky-400 text-3xl mb-4"><i class="bi bi-box-seam"></i></div>
                    <h3 class="text-xl font-bold text-white mb-3">Adapter Architecture</h3>
                    <p class="text-slate-400 text-sm leading-relaxed">
                        Uses an Adapter Pattern to handle OS-specific commands, ensuring robust behavior across Windows, Unix, and Wine.
                    </p>
                </div>
                <!-- Cap 5 -->
                <div class="panel p-6 capability-card">
                    <div class="text-sky-400 text-3xl mb-4"><i class="bi bi-lightning-charge"></i></div>
                    <h3 class="text-xl font-bold text-white mb-3">Streaming Inference</h3>
                    <p class="text-slate-400 text-sm leading-relaxed">
                        Professional-grade latency with streaming responses, making local AI feel as responsive as cloud-based solutions.
                    </p>
                </div>
                <!-- Cap 6 -->
                <div class="panel p-6 capability-card">
                    <div class="text-sky-400 text-3xl mb-4"><i class="bi bi-save"></i></div>
                    <h3 class="text-xl font-bold text-white mb-3">Persistent Configs</h3>
                    <p class="text-slate-400 text-sm leading-relaxed">
                        All model settings, hardware allocations, and UI preferences are saved locally and reloaded on startup.
                    </p>
                </div>
            </div>
        </section>

        <!-- Execution Lifecycle -->
        <section class="mb-20">
            <h2 class="section-title text-3xl font-bold mb-12 text-white text-center">Execution Lifecycle</h2>
            <div class="space-y-8 max-w-4xl mx-auto">
                <div class="flex gap-6 items-center">
                    <div class="step-number">1</div>
                    <div class="panel p-6 flex-grow">
                        <h4 class="text-lg font-bold text-white mb-2">Hardware Probing</h4>
                        <p class="text-slate-400">Identifies hardware capabilities (GPU vs CPU) and selects the appropriate execution strategy for the model.</p>
                    </div>
                </div>
                <div class="flex gap-6 items-center">
                    <div class="step-number">2</div>
                    <div class="panel p-6 flex-grow">
                        <h4 class="text-lg font-bold text-white mb-2">Server Orchestration</h4>
                        <p class="text-slate-400">Spawns a managed <code class="text-sky-400">llama-server</code> subprocess with optimized flags for memory and performance.</p>
                    </div>
                </div>
                <div class="flex gap-6 items-center">
                    <div class="step-number">3</div>
                    <div class="panel p-6 flex-grow">
                        <h4 class="text-lg font-bold text-white mb-2">Health Synchronization</h4>
                        <p class="text-slate-400">Enters a smart polling state, checking health endpoints until the model is fully loaded and ready for prompts.</p>
                    </div>
                </div>
                <div class="flex gap-6 items-center">
                    <div class="step-number">4</div>
                    <div class="panel p-6 flex-grow">
                        <h4 class="text-lg font-bold text-white mb-2">Request Routing</h4>
                        <p class="text-slate-400">Acts as a proxy, transforming Python commands or Web UI interactions into optimized inference requests.</p>
                    </div>
                </div>
                <div class="flex gap-6 items-center">
                    <div class="step-number">5</div>
                    <div class="panel p-6 flex-grow">
                        <h4 class="text-lg font-bold text-white mb-2">Lifecycle Management</h4>
                        <p class="text-slate-400">Ensures graceful shutdown, cleaning up background processes and saving session states reliably.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Mission & Impact -->
        <section class="mb-20">
            <h2 class="section-title text-3xl font-bold mb-10 text-white text-center">Mission & Impact</h2>
            <div class="grid md:grid-cols-2 gap-8">
                <div class="panel p-8">
                    <h3 class="text-xl font-bold text-white mb-4 flex items-center"><i class="bi bi-universal-access mr-3 text-green-500"></i> Democratizing AI</h3>
                    <p class="text-slate-400 leading-relaxed">
                        Lowers the barrier to entry for running powerful AI locally by abstracting infrastructure details into a clean Python API.
                    </p>
                </div>
                <div class="panel p-8">
                    <h3 class="text-xl font-bold text-white mb-4 flex items-center"><i class="bi bi-shield-lock mr-3 text-sky-500"></i> Privacy First</h3>
                    <p class="text-slate-400 leading-relaxed">
                        A "Self-Hosted OpenAI" alternative. Sensitive data never leaves your machine while providing enterprise-grade flexibility.
                    </p>
                </div>
                <div class="panel p-8">
                    <h3 class="text-xl font-bold text-white mb-4 flex items-center"><i class="bi bi-diagram-3 mr-3 text-purple-500"></i> Robust Design</h3>
                    <p class="text-slate-400 leading-relaxed">
                        Utilizes patterns like Facade and Strategy to ensure the system is easily extensible for future hardware and platforms.
                    </p>
                </div>
                <div class="panel p-8">
                    <h3 class="text-xl font-bold text-white mb-4 flex items-center"><i class="bi bi-cloud-slash mr-3 text-amber-500"></i> Local Efficiency</h3>
                    <p class="text-slate-400 leading-relaxed">
                        Optimized architecture reduces memory overhead and maximizes inference speed without requiring cloud connectivity.
                    </p>
                </div>
            </div>
        </section>

    </div>

    <footer class="text-center py-12 border-t border-slate-800">
        <p class="text-sm text-slate-500">&copy; 2026 brendondgr. All rights reserved.</p>
    </footer>

    <script src="/section/javascript/navbar-loader.js"></script>
</body>
</html>
